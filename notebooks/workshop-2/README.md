# Workshop 2

## Regularisation

### Objective

In this workshop, our objective is to implement and test two popular regularisation techniques on a non-linear classifier with the MNIST dataset.

Your job is to experiment with the following:

* L2 regularisation,
* remove L2 regularisation and try random dropout.

Carefully observe the difference between the training set loss and the validation set loss with and without the implementation of regularisation. Make sure you understand why this technique is important and the different ways it can be implemented in the code.

If you would like to experiment with multi-label tasks:

1. The `multi-label` notebook contains an exercise where more labels are added to the MNIST dataset.
2. You can explore the code, think of your own labels to add, and learn how the network had to change in order to do this task. 


